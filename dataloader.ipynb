{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import ast\n",
    "from torchvision.transforms import PILToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDataset(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.dir = \"dataset_resized/\"\n",
    "        self.annotations = pd.read_csv(os.path.join(self.dir, \"annotations.csv\"))\n",
    "\n",
    "        self.X = self.annotations.filename\n",
    "        self.parking_lots = self.annotations.parking_lots\n",
    "        \n",
    "    # def resize_image(self, img_arr, bboxes, h, w):\n",
    "    #     \"\"\"\n",
    "    #     :param img_arr: original image as a numpy array\n",
    "    #     :param bboxes: bboxes as numpy array where each row is 'x_min', 'y_min', 'x_max', 'y_max', \"class_id\"\n",
    "    #     :param h: resized height dimension of image\n",
    "    #     :param w: resized weight dimension of image\n",
    "    #     :return: dictionary containing {image:transformed, bboxes:['x_min', 'y_min', 'x_max', 'y_max', \"class_id\"]}\n",
    "    #     \"\"\"\n",
    "    #     # create resize transform pipeline\n",
    "    #     # transform = albumentations.Compose(\n",
    "    #     #     [albumentations.Resize(height=h, width=w, always_apply=True)],\n",
    "    #     #     bbox_params=albumentations.BboxParams(format='pascal_voc'))\n",
    "\n",
    "    #     # transformed = transform(image=img_arr, bboxes=bboxes)\n",
    "    #     # print(\"img_arr\", img_arr.shape)\n",
    "    #     # return transformed\n",
    "    #     height, width, _ = img_arr.shape\n",
    "    #     moved = np.moveaxis(img_arr, -1, 0)\n",
    "    #     # print(\"moved\", moved.shape)\n",
    "    #     img_arr = torch.from_numpy(moved)\n",
    "    #     transform = Resize(size=(h, w))\n",
    "\n",
    "    #     # print(h, w, height, width)\n",
    "    #     image = transform(img_arr)\n",
    "    #     # print(\"inside resize\", bboxes)\n",
    "    #     # print(\"transformed\", image.shape)\n",
    "    #     try:\n",
    "    #         bboxes[:, 0] = bboxes[:, 0] * (w/width)\n",
    "    #         bboxes[:, 1] = bboxes[:, 1] * (h/height)\n",
    "    #         bboxes[:, 2] = bboxes[:, 2] * (w/width)\n",
    "    #         bboxes[:, 3] = bboxes[:, 3] * (h/height)\n",
    "\n",
    "    #     except Exception as e:\n",
    "    #         bboxes[0] = bboxes[0] * (w/width)\n",
    "    #         bboxes[1] = bboxes[1] * (h/height)\n",
    "    #         bboxes[2] = bboxes[2] * (w/width)\n",
    "    #         bboxes[3] = bboxes[3] * (h/height)\n",
    "    #     transform = ToTensor()\n",
    "    #     bboxes = transform(bboxes)\n",
    "    #     return {\"image\": image, \"bboxes\": bboxes}\n",
    "\n",
    "    def __len__(self):\n",
    "        nsamples = self.X.shape[0]\n",
    "        return nsamples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "            if torch.is_tensor(idx):\n",
    "                idx = idx.tolist()\n",
    "\n",
    "            self.img_name = os.path.join(self.dir,\n",
    "                                    self.X[idx])\n",
    "            self.image = Image.open(self.img_name)\n",
    "            labeled_bbox = self.parking_lots[idx]\n",
    "            labeled_bbox = ast.literal_eval(labeled_bbox)\n",
    "            # print(labeled_bbox)\n",
    "            bboxes = [x[0:-1] for x in labeled_bbox]\n",
    "            labels = [x[-1] for x in labeled_bbox]\n",
    "            transform = PILToTensor()\n",
    "            self.image = transform(self.image)\n",
    "            self.bboxes = torch.tensor(labeled_bbox)\n",
    "            # print(self.bboxes.dtype)\n",
    "            # print(self.bboxes[:, 0])\n",
    "            sample = {'image': self.image, 'bounding_boxes': np.asarray(bboxes), 'labels': labels}\n",
    "            # sample = self.resize_image(img_arr=self.image, bboxes= self.bboxes, h=224, w=224)\n",
    "\n",
    "            return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkingDataset = customDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(parkingDataset, batch_size=8, shuffle=True, collate_fn=lambda x: x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image': tensor([[[208, 198, 168,  ..., 162, 165, 164],\n",
      "         [151, 163, 192,  ..., 159, 162, 164],\n",
      "         [110, 145, 203,  ..., 158, 161, 167],\n",
      "         ...,\n",
      "         [ 33,  33,  33,  ..., 149, 152, 153],\n",
      "         [ 34,  34,  33,  ..., 145, 141, 152],\n",
      "         [ 35,  34,  33,  ..., 154, 147, 165]],\n",
      "\n",
      "        [[204, 199, 178,  ..., 200, 203, 202],\n",
      "         [150, 167, 202,  ..., 197, 200, 202],\n",
      "         [111, 150, 213,  ..., 196, 199, 205],\n",
      "         ...,\n",
      "         [ 33,  33,  33,  ..., 133, 136, 137],\n",
      "         [ 34,  34,  33,  ..., 129, 125, 136],\n",
      "         [ 35,  34,  33,  ..., 138, 131, 149]],\n",
      "\n",
      "        [[237, 229, 203,  ..., 245, 248, 247],\n",
      "         [182, 196, 227,  ..., 242, 245, 247],\n",
      "         [142, 179, 238,  ..., 241, 244, 250],\n",
      "         ...,\n",
      "         [ 33,  33,  33,  ..., 100, 103, 104],\n",
      "         [ 34,  34,  33,  ...,  96,  92, 103],\n",
      "         [ 35,  34,  33,  ..., 105,  98, 116]]], dtype=torch.uint8), 'bounding_boxes': array([[  0.,  57., 148., 216.],\n",
      "       [148.,  86., 224., 188.],\n",
      "       [167.,  55., 209.,  78.],\n",
      "       [143.,  56., 174.,  79.],\n",
      "       [106.,  52., 146.,  83.]]), 'labels': [1, 0, 1, 1, 1]}, {'image': tensor([[[183, 187, 174,  ..., 195, 220, 209],\n",
      "         [126, 181, 198,  ..., 126, 157, 157],\n",
      "         [ 37, 131, 196,  ..., 125, 160, 161],\n",
      "         ...,\n",
      "         [115, 114, 112,  ...,  69,  68,  68],\n",
      "         [121, 119, 118,  ...,  76,  76,  75],\n",
      "         [125, 124, 122,  ...,  77,  76,  76]],\n",
      "\n",
      "        [[186, 189, 178,  ..., 123, 146, 132],\n",
      "         [129, 183, 202,  ...,  70, 100, 100],\n",
      "         [ 39, 133, 199,  ...,  99, 132, 131],\n",
      "         ...,\n",
      "         [115, 114, 112,  ...,  69,  68,  68],\n",
      "         [121, 119, 118,  ...,  78,  78,  77],\n",
      "         [124, 123, 121,  ...,  79,  78,  78]],\n",
      "\n",
      "        [[169, 175, 164,  ..., 137, 161, 148],\n",
      "         [112, 169, 188,  ...,  79, 109, 109],\n",
      "         [ 25, 120, 188,  ...,  98, 131, 131],\n",
      "         ...,\n",
      "         [113, 112, 110,  ...,  67,  66,  66],\n",
      "         [119, 117, 116,  ...,  77,  77,  76],\n",
      "         [122, 121, 119,  ...,  78,  77,  77]]], dtype=torch.uint8), 'bounding_boxes': array([[  0., 127.,  70., 185.],\n",
      "       [  0.,  95.,  94., 156.],\n",
      "       [ 18.,  74., 131., 126.],\n",
      "       [ 74.,  63., 156., 104.],\n",
      "       [ 94.,  50., 183.,  90.],\n",
      "       [113.,  39., 193.,  74.],\n",
      "       [142.,  23., 212.,  58.],\n",
      "       [164.,  14., 223.,  43.],\n",
      "       [  0.,  25.,  15.,  53.],\n",
      "       [  0.,  10.,  36.,  43.],\n",
      "       [ 17.,   5.,  83.,  30.],\n",
      "       [  0.,   0., 124.,  16.],\n",
      "       [ 96., 125., 222., 195.],\n",
      "       [126.,  93., 224., 156.],\n",
      "       [157.,  82., 224., 123.],\n",
      "       [202.,  45., 224.,  91.]]), 'labels': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}]\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    sample = batch\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4])\n",
      "tensor([[  0,  57, 148, 216],\n",
      "        [148,  86, 224, 188],\n",
      "        [167,  55, 209,  78],\n",
      "        [143,  56, 174,  79],\n",
      "        [106,  52, 146,  83]], dtype=torch.uint8)\n",
      "['1', '0', '1', '1', '1']\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.io import read_image\n",
    "import torchvision\n",
    "\n",
    "bboxes = sample[0][\"bounding_boxes\"]\n",
    "bboxes = torch.tensor(bboxes, dtype=torch.uint8)\n",
    "\n",
    "print(bboxes.shape)\n",
    "# bbox = bboxes[:, 0:-1]\n",
    "# labels = bboxes[:, -1]\n",
    "# labels = [str(l) for l in labels]\n",
    "# print(bbox)\n",
    "labels = sample[0][\"labels\"]\n",
    "labels = [str(x) for x in labels]\n",
    "# bbox = bbox.unsqueeze(0)\n",
    "print(bboxes)\n",
    "\n",
    "img = sample[0][\"image\"]\n",
    "# img = img.permute(2, 0, 1)\n",
    "# img = torch.transpose(img, 2, 0, 1)\n",
    "print(labels)\n",
    "print(img.shape)\n",
    "# img = torch.tensor(img, dtype=torch.uint8)\n",
    "img = draw_bounding_boxes(img, bboxes,\n",
    "                        colors=\"green\",\n",
    "                        labels=labels)\n",
    "\n",
    "# transform this image to PIL image \n",
    "img = torchvision.transforms.ToPILImage()(img) \n",
    "\n",
    "# display output \n",
    "img.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16316\\1926653515.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "sample[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 97.,   0., 175.,  41.],\n",
       "       [ 95.,  55., 157.,  92.],\n",
       "       [ 93., 113., 152., 141.],\n",
       "       [ 92., 146., 170., 167.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]['bounding_boxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsci6004",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
